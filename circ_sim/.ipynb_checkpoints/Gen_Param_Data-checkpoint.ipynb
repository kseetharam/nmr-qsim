{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation of basis, jump operators and damping rates in latex format, as well as generation of NOESY spectra and FID\n",
    "\n",
    "import sys\n",
    "sys.path.append('./utils/')\n",
    "import numpy as np\n",
    "import scipy.io as spio\n",
    "from scipy.linalg import expm\n",
    "import sys \n",
    "import json\n",
    "#import pandas as pd\n",
    "\n",
    "from basis_utils import read_spinach_info, build_list_ISTs, NormalizeBasis, build_symbolic_list_ISTs\n",
    "from analytical_fit import get_chemical_shifts, Get_Det_And_Rates_latex\n",
    "from simulation_utils import GenNOESYSpectrum\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Functions to construct and format text in latex format...\n",
    "import math\n",
    "import re\n",
    "\n",
    "def round_up(number, decimals=0):\n",
    "    factor = 10 ** decimals\n",
    "    return math.ceil(number * factor) / factor\n",
    "\n",
    "\n",
    "def Gen_diss_param_latex_table(list_jumps,list_symb_rates,list_damp_rates,mol_str,ndecs=2):\n",
    "    \"\"\"\n",
    "    Generation of a latex table that uses the longtable environment \n",
    "    Args:\n",
    "    list_jumps, list of jump operators in latex format\n",
    "    list_symb_rates, list of symbolic rates in latex format\n",
    "    list_damp_rates, list fo damping rates\n",
    "    mol_str, a string to identify the molecule\n",
    "    \"\"\"\n",
    "\n",
    "    Nentries = len(list_jumps)\n",
    "    lines = []\n",
    "    lines.append(r'\\begin{longtable}{|c|c|c|}')\n",
    "    lines.append('\\\\caption{Jump operators and their associated damping rates for'+mol_str+'} \\\\\\\\')\n",
    "    lines.append('\\hline')\n",
    "    lines.append(r'\\textbf{$L_j$} & \\textbf{$\\kappa_j$} & \\textbf{Value (Hz)} \\\\')\n",
    "    lines.append(r'\\hline')\n",
    "    lines.append(r'\\endhead')\n",
    "\n",
    "    for i in range(Nentries):\n",
    "        line = '$'+list_jumps[i]+'$'+' & '+'$'+list_symb_rates[i]+'$'+ ' & '+str(round_up(list_damp_rates[i],ndecs))+'\\\\\\\\\\\\hline'\n",
    "        lines.append(line)\n",
    "        #list_jumps\n",
    "\n",
    "    lines.append(r'\\end{longtable}')\n",
    "    text_to_save = \"\\n\".join(lines)\n",
    "\n",
    "    return text_to_save\n",
    "\n",
    "\n",
    "def basis_to_latex_format(arr):\n",
    "    latex_list = []\n",
    "    \n",
    "    # Regex to capture 'S', an optional superscript (+, -, or z), and a digit index.\n",
    "    pattern = r\"S([+-z]?)(\\d)\"\n",
    "    \n",
    "    for entry in arr:\n",
    "        # Find all matches of S terms in each entry\n",
    "        matches = re.findall(pattern, entry)\n",
    "        \n",
    "        # Convert each match to the LaTeX format\n",
    "        latex_terms = []\n",
    "        for superscript, index in matches:\n",
    "            if superscript == \"z\":\n",
    "                latex_term = f\"S_{{{index}z}}\"\n",
    "            elif superscript == \"+\":\n",
    "                latex_term = f\"S_{{{index}+}}\"\n",
    "            elif superscript == \"-\":\n",
    "                latex_term = f\"S_{{{index}-}}\"\n",
    "            else:\n",
    "                latex_term = f\"S_{{{index}}}\"\n",
    "            latex_terms.append(latex_term)\n",
    "        \n",
    "        # Join terms with spaces and wrap in $ for LaTeX format\n",
    "        latex_str = \"$\" + \" \".join(latex_terms) + \"$\"\n",
    "        latex_list.append(latex_str)\n",
    "    \n",
    "    return latex_list\n",
    "\n",
    "\n",
    "def formattedlist_to_latextable(latex_list,mol_str):\n",
    "    \"\"\"\n",
    "    Generation of a latex table that uses the longtable environment \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    Nbasis = len(latex_list)\n",
    "    lines = []\n",
    "\n",
    "    lines.append(r'\\begin{longtable}{|c|c|}')\n",
    "    lines.append(r'\\caption{Basis operators for '+mol_str+' } \\\\')\n",
    "    lines.append(r'\\hline')\n",
    "    lines.append(r'\\textbf{Operator} & \\textbf{Index}\\\\')\n",
    "    lines.append(r'\\hline')\n",
    "    lines.append(r'\\endhead')\n",
    "\n",
    "    for i in range(Nbasis):\n",
    "        line = latex_list[i]+' & '+str(i)+' \\\\\\\\\\\\hline'\n",
    "        lines.append(line)\n",
    "\n",
    "    lines.append(r'\\end{longtable}')\n",
    "\n",
    "    text_to_save = \"\\n\".join(lines)\n",
    "\n",
    "    return text_to_save\n",
    "\n",
    "\n",
    "def basis_to_latextable(arr,mol_str):\n",
    "    latex_list = basis_to_latex_format(arr)\n",
    "    \n",
    "    return formattedlist_to_latextable(latex_list,mol_str)\n",
    "\n",
    "\n",
    "\n",
    "def filter_jumps(list_jumps,list_symb_rates,list_damp_rates,list_dets,thresh):\n",
    "    filt_jump_ops = []\n",
    "    filt_damp_rates = []\n",
    "    filt_symb_rates = []\n",
    "    thresh = 370\n",
    "    for i in range(len(list_dets)):\n",
    "        if np.abs(list_dets[i])<thresh:\n",
    "            filt_jump_ops.append(list_jumps[i])\n",
    "            filt_symb_rates.append(list_symb_rates[i])\n",
    "            filt_damp_rates.append(list_damp_rates[i])\n",
    "\n",
    "    return filt_jump_ops, filt_symb_rates, filt_damp_rates\n",
    "\n",
    "def SaveNOESYData(tmix,t_grid1,t_grid2,rho0,Ham,R,FID,path):\n",
    "    \"\"\"\n",
    "    path corresponds to the path where the data is saved... \n",
    "    \"\"\"\n",
    "    data = {\n",
    "    \"constants\": {\n",
    "        \"tmix\": tmix\n",
    "    },\n",
    "    \"vectors\": {\n",
    "    \"t_grid1\": np.array(np.real(t_grid1)).tolist(),\n",
    "    \"t_grid2\": np.array(np.real(t_grid2)).tolist(),\n",
    "    \"rho0\": np.array(np.real(rho0)).tolist(),\n",
    "    },\n",
    "    \"matrices\": {\n",
    "        \"Ham\":np.array(np.real(Ham)).tolist(),\n",
    "        \"R\": np.array(np.real(R)).tolist(),\n",
    "        \"FID_real\":np.array(np.real(FID)).tolist(),\n",
    "        \"FID_imag\": np.array(np.imag(FID)).tolist()\n",
    "    }\n",
    "    }\n",
    "\n",
    "    # Save to JSON file\n",
    "    with open(path, \"w\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DFG case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#translation of basis format in Spinach to symbolic form\n",
    "text=\"\"\"1      (0,0)   (0,0)   \n",
    "  2      (0,0)   (1,1)   \n",
    "  3      (0,0)   (1,0)   \n",
    "  4      (0,0)   (1,-1)  \n",
    "  5      (1,1)   (0,0)   \n",
    "  6      (1,1)   (1,1)   \n",
    "  7      (1,1)   (1,0)   \n",
    "  8      (1,1)   (1,-1)  \n",
    "  9      (1,0)   (0,0)   \n",
    "  10     (1,0)   (1,1)   \n",
    "  11     (1,0)   (1,0)   \n",
    "  12     (1,0)   (1,-1)  \n",
    "  13     (1,-1)  (0,0)   \n",
    "  14     (1,-1)  (1,1)   \n",
    "  15     (1,-1)  (1,0)   \n",
    "  16     (1,-1)  (1,-1)  \n",
    "\"\"\"\n",
    "\n",
    "data = read_spinach_info(text)\n",
    "\n",
    "basis = build_list_ISTs(data)\n",
    "prefacts,Symb_basis = build_symbolic_list_ISTs(data)\n",
    "\n",
    "#Normbasis = NormalizeBasis(basis,n_qubits=4,checkOrth=True) I have verified the orthonormalization of the basis\n",
    "Normbasis = NormalizeBasis(basis,n_qubits=4,checkOrth=False)\n",
    "Normbasis = np.array(Normbasis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parameters of the model for determination of jump operators and their associated damping rates...\n",
    "\n",
    "gammaF = 251814800\n",
    "coord1 = np.array([-0.0551,-1.2087,-1.6523])*1e-10\n",
    "coord2 = np.array([-0.8604 ,-2.3200 ,-0.0624])*1e-10\n",
    "\n",
    "coords = np.array([coord1,coord2])\n",
    "\n",
    "w1 = -376417768.6316 \n",
    "w2 = -376411775.1523 \n",
    "freqs = np.array([w1,w2])\n",
    "tc = 0.5255e-9\n",
    "B0 = 9.3933\n",
    "\n",
    "zeeman_scalar_1 = -113.8796\n",
    "zeeman_scalar_2 = -129.8002\n",
    "zeeman_scalars = [zeeman_scalar_1,zeeman_scalar_2]\n",
    "\n",
    "#w0*zeeman_scalars[i]/1e6\n",
    "chem_shifts = get_chemical_shifts(gammaF,B0,zeeman_scalars)\n",
    "Nspins = 2\n",
    "\n",
    "list_jumps, list_symb_rates, list_damp_rates, list_dets = Get_Det_And_Rates_latex(2*np.pi*freqs,tc,coords,Nspins,gammaF,chem_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter jump operators and rates...\n",
    "filt_jumps,filt_symb_rates,filt_rates = filter_jumps(list_jumps,list_symb_rates,list_damp_rates,list_dets,thresh=1e-6)\n",
    "\n",
    "\n",
    "text_dfg_diss = Gen_diss_param_latex_table(filt_jumps,filt_symb_rates,filt_rates,'DFG',ndecs=3)\n",
    "\n",
    "text_dfg_basis = basis_to_latextable(Symb_basis,'DFG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "###saving in text files...\n",
    "with open(\"./data/jumps_dfg.txt\", \"w\") as file:\n",
    "    file.write(text_dfg_diss)\n",
    "\n",
    "with open(\"./data/basis_dfg.txt\", \"w\") as file:\n",
    "    file.write(text_dfg_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generation and saving of NOESY data...\n",
    "loadMat = spio.loadmat('./data/DFG.mat',squeeze_me=True)\n",
    "AuxMats = spio.loadmat('./data/DFG_NOESYmatrices.mat',squeeze_me=True)\n",
    "\n",
    "###These are the parameters generated by Spinach\n",
    "Ham = loadMat['p']['H'].item()\n",
    "R = loadMat['p']['R'].item() # We can substitute this by any approximation to the relaxation matrix\n",
    "t_grid1 = loadMat['p']['time_grid1'].item()\n",
    "t_grid2 = loadMat['p']['time_grid2'].item()\n",
    "\n",
    "###Dynamical evolution for calculation of 2D spectra...\n",
    "##TODO: we can simply modify the script to incorporate 1) retrieval of synthesized circuits,\n",
    "#and 2) the already-developed circuit simulator\n",
    "\n",
    "Tpts1 = len(t_grid1)\n",
    "Tpts2 = len(t_grid2)\n",
    "\n",
    "Dim = Ham.shape[0]\n",
    "\n",
    "rho0 = np.array(AuxMats['rho0'].toarray())\n",
    "rho0 = rho0.flatten()\n",
    "coil = AuxMats['coil']\n",
    "\n",
    "\n",
    "tmix = 0.5\n",
    "dt1 = 1.1561e-04\n",
    "dt2 = 1.1561e-04\n",
    "\n",
    "##Parameters for Fourier transform\n",
    "zerofill1 = 1024\n",
    "zerofill2 = 1024\n",
    "\n",
    "##Definition of pulses in the experiment...\n",
    "#This also depends on the definition of the basis....\n",
    "Lx = AuxMats['Lx'].toarray()\n",
    "Ly = AuxMats['Ly'].toarray()\n",
    "\n",
    "Spectrum, FID = GenNOESYSpectrum(Ham,R,Tpts1,Tpts2,rho0,coil,tmix,dt1,dt2,zerofill1,zerofill2,Lx,Ly)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save noesy data as a JSON file\n",
    "SaveNOESYData(tmix,t_grid1,t_grid2,rho0,Ham,R,FID,'./data/DFG_NOESY_dat.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Alanine case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "text4=\"\"\"1      (0,0)   (0,0)   (0,0)   (0,0)   \n",
    "  2      (0,0)   (0,0)   (0,0)   (1,1)   \n",
    "  3      (0,0)   (0,0)   (0,0)   (1,0)   \n",
    "  4      (0,0)   (0,0)   (0,0)   (1,-1)  \n",
    "  5      (0,0)   (0,0)   (1,1)   (0,0)   \n",
    "  6      (0,0)   (0,0)   (1,1)   (1,1)   \n",
    "  7      (0,0)   (0,0)   (1,1)   (1,0)   \n",
    "  8      (0,0)   (0,0)   (1,1)   (1,-1)  \n",
    "  9      (0,0)   (0,0)   (1,0)   (0,0)   \n",
    "  10     (0,0)   (0,0)   (1,0)   (1,1)   \n",
    "  11     (0,0)   (0,0)   (1,0)   (1,0)   \n",
    "  12     (0,0)   (0,0)   (1,0)   (1,-1)  \n",
    "  13     (0,0)   (0,0)   (1,-1)  (0,0)   \n",
    "  14     (0,0)   (0,0)   (1,-1)  (1,1)   \n",
    "  15     (0,0)   (0,0)   (1,-1)  (1,0)   \n",
    "  16     (0,0)   (0,0)   (1,-1)  (1,-1)  \n",
    "  17     (0,0)   (1,1)   (0,0)   (0,0)   \n",
    "  18     (0,0)   (1,1)   (0,0)   (1,1)   \n",
    "  19     (0,0)   (1,1)   (0,0)   (1,0)   \n",
    "  20     (0,0)   (1,1)   (0,0)   (1,-1)  \n",
    "  21     (0,0)   (1,1)   (1,1)   (0,0)   \n",
    "  22     (0,0)   (1,1)   (1,1)   (1,1)   \n",
    "  23     (0,0)   (1,1)   (1,1)   (1,0)   \n",
    "  24     (0,0)   (1,1)   (1,1)   (1,-1)  \n",
    "  25     (0,0)   (1,1)   (1,0)   (0,0)   \n",
    "  26     (0,0)   (1,1)   (1,0)   (1,1)   \n",
    "  27     (0,0)   (1,1)   (1,0)   (1,0)   \n",
    "  28     (0,0)   (1,1)   (1,0)   (1,-1)  \n",
    "  29     (0,0)   (1,1)   (1,-1)  (0,0)   \n",
    "  30     (0,0)   (1,1)   (1,-1)  (1,1)   \n",
    "  31     (0,0)   (1,1)   (1,-1)  (1,0)   \n",
    "  32     (0,0)   (1,1)   (1,-1)  (1,-1)  \n",
    "  33     (0,0)   (1,0)   (0,0)   (0,0)   \n",
    "  34     (0,0)   (1,0)   (0,0)   (1,1)   \n",
    "  35     (0,0)   (1,0)   (0,0)   (1,0)   \n",
    "  36     (0,0)   (1,0)   (0,0)   (1,-1)  \n",
    "  37     (0,0)   (1,0)   (1,1)   (0,0)   \n",
    "  38     (0,0)   (1,0)   (1,1)   (1,1)   \n",
    "  39     (0,0)   (1,0)   (1,1)   (1,0)   \n",
    "  40     (0,0)   (1,0)   (1,1)   (1,-1)  \n",
    "  41     (0,0)   (1,0)   (1,0)   (0,0)   \n",
    "  42     (0,0)   (1,0)   (1,0)   (1,1)   \n",
    "  43     (0,0)   (1,0)   (1,0)   (1,0)   \n",
    "  44     (0,0)   (1,0)   (1,0)   (1,-1)  \n",
    "  45     (0,0)   (1,0)   (1,-1)  (0,0)   \n",
    "  46     (0,0)   (1,0)   (1,-1)  (1,1)   \n",
    "  47     (0,0)   (1,0)   (1,-1)  (1,0)   \n",
    "  48     (0,0)   (1,0)   (1,-1)  (1,-1)  \n",
    "  49     (0,0)   (1,-1)  (0,0)   (0,0)   \n",
    "  50     (0,0)   (1,-1)  (0,0)   (1,1)   \n",
    "  51     (0,0)   (1,-1)  (0,0)   (1,0)   \n",
    "  52     (0,0)   (1,-1)  (0,0)   (1,-1)  \n",
    "  53     (0,0)   (1,-1)  (1,1)   (0,0)   \n",
    "  54     (0,0)   (1,-1)  (1,1)   (1,1)   \n",
    "  55     (0,0)   (1,-1)  (1,1)   (1,0)   \n",
    "  56     (0,0)   (1,-1)  (1,1)   (1,-1)  \n",
    "  57     (0,0)   (1,-1)  (1,0)   (0,0)   \n",
    "  58     (0,0)   (1,-1)  (1,0)   (1,1)   \n",
    "  59     (0,0)   (1,-1)  (1,0)   (1,0)   \n",
    "  60     (0,0)   (1,-1)  (1,0)   (1,-1)  \n",
    "  61     (0,0)   (1,-1)  (1,-1)  (0,0)   \n",
    "  62     (0,0)   (1,-1)  (1,-1)  (1,1)   \n",
    "  63     (0,0)   (1,-1)  (1,-1)  (1,0)   \n",
    "  64     (0,0)   (1,-1)  (1,-1)  (1,-1)  \n",
    "  65     (1,1)   (0,0)   (0,0)   (0,0)   \n",
    "  66     (1,1)   (0,0)   (0,0)   (1,1)   \n",
    "  67     (1,1)   (0,0)   (0,0)   (1,0)   \n",
    "  68     (1,1)   (0,0)   (0,0)   (1,-1)  \n",
    "  69     (1,1)   (0,0)   (1,1)   (0,0)   \n",
    "  70     (1,1)   (0,0)   (1,1)   (1,1)   \n",
    "  71     (1,1)   (0,0)   (1,1)   (1,0)   \n",
    "  72     (1,1)   (0,0)   (1,1)   (1,-1)  \n",
    "  73     (1,1)   (0,0)   (1,0)   (0,0)   \n",
    "  74     (1,1)   (0,0)   (1,0)   (1,1)   \n",
    "  75     (1,1)   (0,0)   (1,0)   (1,0)   \n",
    "  76     (1,1)   (0,0)   (1,0)   (1,-1)  \n",
    "  77     (1,1)   (0,0)   (1,-1)  (0,0)   \n",
    "  78     (1,1)   (0,0)   (1,-1)  (1,1)   \n",
    "  79     (1,1)   (0,0)   (1,-1)  (1,0)   \n",
    "  80     (1,1)   (0,0)   (1,-1)  (1,-1)  \n",
    "  81     (1,1)   (1,1)   (0,0)   (0,0)   \n",
    "  82     (1,1)   (1,1)   (0,0)   (1,1)   \n",
    "  83     (1,1)   (1,1)   (0,0)   (1,0)   \n",
    "  84     (1,1)   (1,1)   (0,0)   (1,-1)  \n",
    "  85     (1,1)   (1,1)   (1,1)   (0,0)   \n",
    "  86     (1,1)   (1,1)   (1,1)   (1,1)   \n",
    "  87     (1,1)   (1,1)   (1,1)   (1,0)   \n",
    "  88     (1,1)   (1,1)   (1,1)   (1,-1)  \n",
    "  89     (1,1)   (1,1)   (1,0)   (0,0)   \n",
    "  90     (1,1)   (1,1)   (1,0)   (1,1)   \n",
    "  91     (1,1)   (1,1)   (1,0)   (1,0)   \n",
    "  92     (1,1)   (1,1)   (1,0)   (1,-1)  \n",
    "  93     (1,1)   (1,1)   (1,-1)  (0,0)   \n",
    "  94     (1,1)   (1,1)   (1,-1)  (1,1)   \n",
    "  95     (1,1)   (1,1)   (1,-1)  (1,0)   \n",
    "  96     (1,1)   (1,1)   (1,-1)  (1,-1)  \n",
    "  97     (1,1)   (1,0)   (0,0)   (0,0)   \n",
    "  98     (1,1)   (1,0)   (0,0)   (1,1)   \n",
    "  99     (1,1)   (1,0)   (0,0)   (1,0)   \n",
    "  100    (1,1)   (1,0)   (0,0)   (1,-1)  \n",
    "  101    (1,1)   (1,0)   (1,1)   (0,0)   \n",
    "  102    (1,1)   (1,0)   (1,1)   (1,1)   \n",
    "  103    (1,1)   (1,0)   (1,1)   (1,0)   \n",
    "  104    (1,1)   (1,0)   (1,1)   (1,-1)  \n",
    "  105    (1,1)   (1,0)   (1,0)   (0,0)   \n",
    "  106    (1,1)   (1,0)   (1,0)   (1,1)   \n",
    "  107    (1,1)   (1,0)   (1,0)   (1,0)   \n",
    "  108    (1,1)   (1,0)   (1,0)   (1,-1)  \n",
    "  109    (1,1)   (1,0)   (1,-1)  (0,0)   \n",
    "  110    (1,1)   (1,0)   (1,-1)  (1,1)   \n",
    "  111    (1,1)   (1,0)   (1,-1)  (1,0)   \n",
    "  112    (1,1)   (1,0)   (1,-1)  (1,-1)  \n",
    "  113    (1,1)   (1,-1)  (0,0)   (0,0)   \n",
    "  114    (1,1)   (1,-1)  (0,0)   (1,1)   \n",
    "  115    (1,1)   (1,-1)  (0,0)   (1,0)   \n",
    "  116    (1,1)   (1,-1)  (0,0)   (1,-1)  \n",
    "  117    (1,1)   (1,-1)  (1,1)   (0,0)   \n",
    "  118    (1,1)   (1,-1)  (1,1)   (1,1)   \n",
    "  119    (1,1)   (1,-1)  (1,1)   (1,0)   \n",
    "  120    (1,1)   (1,-1)  (1,1)   (1,-1)  \n",
    "  121    (1,1)   (1,-1)  (1,0)   (0,0)   \n",
    "  122    (1,1)   (1,-1)  (1,0)   (1,1)   \n",
    "  123    (1,1)   (1,-1)  (1,0)   (1,0)   \n",
    "  124    (1,1)   (1,-1)  (1,0)   (1,-1)  \n",
    "  125    (1,1)   (1,-1)  (1,-1)  (0,0)   \n",
    "  126    (1,1)   (1,-1)  (1,-1)  (1,1)   \n",
    "  127    (1,1)   (1,-1)  (1,-1)  (1,0)   \n",
    "  128    (1,1)   (1,-1)  (1,-1)  (1,-1)  \n",
    "  129    (1,0)   (0,0)   (0,0)   (0,0)   \n",
    "  130    (1,0)   (0,0)   (0,0)   (1,1)   \n",
    "  131    (1,0)   (0,0)   (0,0)   (1,0)   \n",
    "  132    (1,0)   (0,0)   (0,0)   (1,-1)  \n",
    "  133    (1,0)   (0,0)   (1,1)   (0,0)   \n",
    "  134    (1,0)   (0,0)   (1,1)   (1,1)   \n",
    "  135    (1,0)   (0,0)   (1,1)   (1,0)   \n",
    "  136    (1,0)   (0,0)   (1,1)   (1,-1)  \n",
    "  137    (1,0)   (0,0)   (1,0)   (0,0)   \n",
    "  138    (1,0)   (0,0)   (1,0)   (1,1)   \n",
    "  139    (1,0)   (0,0)   (1,0)   (1,0)   \n",
    "  140    (1,0)   (0,0)   (1,0)   (1,-1)  \n",
    "  141    (1,0)   (0,0)   (1,-1)  (0,0)   \n",
    "  142    (1,0)   (0,0)   (1,-1)  (1,1)   \n",
    "  143    (1,0)   (0,0)   (1,-1)  (1,0)   \n",
    "  144    (1,0)   (0,0)   (1,-1)  (1,-1)  \n",
    "  145    (1,0)   (1,1)   (0,0)   (0,0)   \n",
    "  146    (1,0)   (1,1)   (0,0)   (1,1)   \n",
    "  147    (1,0)   (1,1)   (0,0)   (1,0)   \n",
    "  148    (1,0)   (1,1)   (0,0)   (1,-1)  \n",
    "  149    (1,0)   (1,1)   (1,1)   (0,0)   \n",
    "  150    (1,0)   (1,1)   (1,1)   (1,1)   \n",
    "  151    (1,0)   (1,1)   (1,1)   (1,0)   \n",
    "  152    (1,0)   (1,1)   (1,1)   (1,-1)  \n",
    "  153    (1,0)   (1,1)   (1,0)   (0,0)   \n",
    "  154    (1,0)   (1,1)   (1,0)   (1,1)   \n",
    "  155    (1,0)   (1,1)   (1,0)   (1,0)   \n",
    "  156    (1,0)   (1,1)   (1,0)   (1,-1)  \n",
    "  157    (1,0)   (1,1)   (1,-1)  (0,0)   \n",
    "  158    (1,0)   (1,1)   (1,-1)  (1,1)   \n",
    "  159    (1,0)   (1,1)   (1,-1)  (1,0)   \n",
    "  160    (1,0)   (1,1)   (1,-1)  (1,-1)  \n",
    "  161    (1,0)   (1,0)   (0,0)   (0,0)   \n",
    "  162    (1,0)   (1,0)   (0,0)   (1,1)   \n",
    "  163    (1,0)   (1,0)   (0,0)   (1,0)   \n",
    "  164    (1,0)   (1,0)   (0,0)   (1,-1)  \n",
    "  165    (1,0)   (1,0)   (1,1)   (0,0)   \n",
    "  166    (1,0)   (1,0)   (1,1)   (1,1)   \n",
    "  167    (1,0)   (1,0)   (1,1)   (1,0)   \n",
    "  168    (1,0)   (1,0)   (1,1)   (1,-1)  \n",
    "  169    (1,0)   (1,0)   (1,0)   (0,0)   \n",
    "  170    (1,0)   (1,0)   (1,0)   (1,1)   \n",
    "  171    (1,0)   (1,0)   (1,0)   (1,0)   \n",
    "  172    (1,0)   (1,0)   (1,0)   (1,-1)  \n",
    "  173    (1,0)   (1,0)   (1,-1)  (0,0)   \n",
    "  174    (1,0)   (1,0)   (1,-1)  (1,1)   \n",
    "  175    (1,0)   (1,0)   (1,-1)  (1,0)   \n",
    "  176    (1,0)   (1,0)   (1,-1)  (1,-1)  \n",
    "  177    (1,0)   (1,-1)  (0,0)   (0,0)   \n",
    "  178    (1,0)   (1,-1)  (0,0)   (1,1)   \n",
    "  179    (1,0)   (1,-1)  (0,0)   (1,0)   \n",
    "  180    (1,0)   (1,-1)  (0,0)   (1,-1)  \n",
    "  181    (1,0)   (1,-1)  (1,1)   (0,0)   \n",
    "  182    (1,0)   (1,-1)  (1,1)   (1,1)   \n",
    "  183    (1,0)   (1,-1)  (1,1)   (1,0)   \n",
    "  184    (1,0)   (1,-1)  (1,1)   (1,-1)  \n",
    "  185    (1,0)   (1,-1)  (1,0)   (0,0)   \n",
    "  186    (1,0)   (1,-1)  (1,0)   (1,1)   \n",
    "  187    (1,0)   (1,-1)  (1,0)   (1,0)   \n",
    "  188    (1,0)   (1,-1)  (1,0)   (1,-1)  \n",
    "  189    (1,0)   (1,-1)  (1,-1)  (0,0)   \n",
    "  190    (1,0)   (1,-1)  (1,-1)  (1,1)   \n",
    "  191    (1,0)   (1,-1)  (1,-1)  (1,0)   \n",
    "  192    (1,0)   (1,-1)  (1,-1)  (1,-1)  \n",
    "  193    (1,-1)  (0,0)   (0,0)   (0,0)   \n",
    "  194    (1,-1)  (0,0)   (0,0)   (1,1)   \n",
    "  195    (1,-1)  (0,0)   (0,0)   (1,0)   \n",
    "  196    (1,-1)  (0,0)   (0,0)   (1,-1)  \n",
    "  197    (1,-1)  (0,0)   (1,1)   (0,0)   \n",
    "  198    (1,-1)  (0,0)   (1,1)   (1,1)   \n",
    "  199    (1,-1)  (0,0)   (1,1)   (1,0)   \n",
    "  200    (1,-1)  (0,0)   (1,1)   (1,-1)  \n",
    "  201    (1,-1)  (0,0)   (1,0)   (0,0)   \n",
    "  202    (1,-1)  (0,0)   (1,0)   (1,1)   \n",
    "  203    (1,-1)  (0,0)   (1,0)   (1,0)   \n",
    "  204    (1,-1)  (0,0)   (1,0)   (1,-1)  \n",
    "  205    (1,-1)  (0,0)   (1,-1)  (0,0)   \n",
    "  206    (1,-1)  (0,0)   (1,-1)  (1,1)   \n",
    "  207    (1,-1)  (0,0)   (1,-1)  (1,0)   \n",
    "  208    (1,-1)  (0,0)   (1,-1)  (1,-1)  \n",
    "  209    (1,-1)  (1,1)   (0,0)   (0,0)   \n",
    "  210    (1,-1)  (1,1)   (0,0)   (1,1)   \n",
    "  211    (1,-1)  (1,1)   (0,0)   (1,0)   \n",
    "  212    (1,-1)  (1,1)   (0,0)   (1,-1)  \n",
    "  213    (1,-1)  (1,1)   (1,1)   (0,0)   \n",
    "  214    (1,-1)  (1,1)   (1,1)   (1,1)   \n",
    "  215    (1,-1)  (1,1)   (1,1)   (1,0)   \n",
    "  216    (1,-1)  (1,1)   (1,1)   (1,-1)  \n",
    "  217    (1,-1)  (1,1)   (1,0)   (0,0)   \n",
    "  218    (1,-1)  (1,1)   (1,0)   (1,1)   \n",
    "  219    (1,-1)  (1,1)   (1,0)   (1,0)   \n",
    "  220    (1,-1)  (1,1)   (1,0)   (1,-1)  \n",
    "  221    (1,-1)  (1,1)   (1,-1)  (0,0)   \n",
    "  222    (1,-1)  (1,1)   (1,-1)  (1,1)   \n",
    "  223    (1,-1)  (1,1)   (1,-1)  (1,0)   \n",
    "  224    (1,-1)  (1,1)   (1,-1)  (1,-1)  \n",
    "  225    (1,-1)  (1,0)   (0,0)   (0,0)   \n",
    "  226    (1,-1)  (1,0)   (0,0)   (1,1)   \n",
    "  227    (1,-1)  (1,0)   (0,0)   (1,0)   \n",
    "  228    (1,-1)  (1,0)   (0,0)   (1,-1)  \n",
    "  229    (1,-1)  (1,0)   (1,1)   (0,0)   \n",
    "  230    (1,-1)  (1,0)   (1,1)   (1,1)   \n",
    "  231    (1,-1)  (1,0)   (1,1)   (1,0)   \n",
    "  232    (1,-1)  (1,0)   (1,1)   (1,-1)  \n",
    "  233    (1,-1)  (1,0)   (1,0)   (0,0)   \n",
    "  234    (1,-1)  (1,0)   (1,0)   (1,1)   \n",
    "  235    (1,-1)  (1,0)   (1,0)   (1,0)   \n",
    "  236    (1,-1)  (1,0)   (1,0)   (1,-1)  \n",
    "  237    (1,-1)  (1,0)   (1,-1)  (0,0)   \n",
    "  238    (1,-1)  (1,0)   (1,-1)  (1,1)   \n",
    "  239    (1,-1)  (1,0)   (1,-1)  (1,0)   \n",
    "  240    (1,-1)  (1,0)   (1,-1)  (1,-1)  \n",
    "  241    (1,-1)  (1,-1)  (0,0)   (0,0)   \n",
    "  242    (1,-1)  (1,-1)  (0,0)   (1,1)   \n",
    "  243    (1,-1)  (1,-1)  (0,0)   (1,0)   \n",
    "  244    (1,-1)  (1,-1)  (0,0)   (1,-1)  \n",
    "  245    (1,-1)  (1,-1)  (1,1)   (0,0)   \n",
    "  246    (1,-1)  (1,-1)  (1,1)   (1,1)   \n",
    "  247    (1,-1)  (1,-1)  (1,1)   (1,0)   \n",
    "  248    (1,-1)  (1,-1)  (1,1)   (1,-1)  \n",
    "  249    (1,-1)  (1,-1)  (1,0)   (0,0)   \n",
    "  250    (1,-1)  (1,-1)  (1,0)   (1,1)   \n",
    "  251    (1,-1)  (1,-1)  (1,0)   (1,0)   \n",
    "  252    (1,-1)  (1,-1)  (1,0)   (1,-1)  \n",
    "  253    (1,-1)  (1,-1)  (1,-1)  (0,0)   \n",
    "  254    (1,-1)  (1,-1)  (1,-1)  (1,1)   \n",
    "  255    (1,-1)  (1,-1)  (1,-1)  (1,0)   \n",
    "  256    (1,-1)  (1,-1)  (1,-1)  (1,-1)\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_ala = read_spinach_info(text4)\n",
    "\n",
    "basis_ala = build_list_ISTs(data_ala)\n",
    "prefacts,Symb_ALA_basis = build_symbolic_list_ISTs(data_ala)\n",
    "\n",
    "#Normbasis = NormalizeBasis(basis,n_qubits=4,checkOrth=True) I have verified the orthonormalization of the basis\n",
    "Normbasis_ala = NormalizeBasis(basis_ala,n_qubits=4,checkOrth=False)\n",
    "Normbasis_ala = np.array(Normbasis_ala)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = 0.05e-9 # in seconds\n",
    "gammaH = 2.6752e8\n",
    "B0 =14.1\n",
    "Nspins = 4\n",
    "\n",
    "w1 = -600344544.5579\n",
    "w2 = -600343524.536\n",
    "w3 = -600343524.536\n",
    "w4 = -600343524.536\n",
    "\n",
    "freqs = np.array([w1,w2,w3,w4])\n",
    "\n",
    "coords = np.array([\n",
    "    [ 0.6861,0.2705,1.5010],\n",
    "    [1.3077, 1.1298, -1.3993],\n",
    "    [0.7905, 2.2125, -0.0860],\n",
    "    [ 2.3693, 1.3798, 0.0233]\n",
    "])\n",
    "\n",
    "coords = coords*1e-10\n",
    "\n",
    "#isotropic chemical shifts in ppm taken from the chemical shift tensors introduced in Spinach\n",
    "zeeman_scalar_1 = 3.4938\n",
    "zeeman_scalar_2 = 1.7947\n",
    "zeeman_scalar_3 = 1.7947\n",
    "zeeman_scalar_4 = 1.7947\n",
    "\n",
    "zeeman_scalars = [zeeman_scalar_1,zeeman_scalar_2,zeeman_scalar_3,zeeman_scalar_4]\n",
    "\n",
    "chem_shifts = get_chemical_shifts(gammaH,B0,zeeman_scalars)\n",
    "\n",
    "\n",
    "list_jumps, list_symb_rates, list_damp_rates, list_dets = Get_Det_And_Rates_latex(2*np.pi*freqs,tc,coords,Nspins,gammaF,chem_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_jumps,filt_symb_rates,filt_rates = filter_jumps(list_jumps,list_symb_rates,list_damp_rates,list_dets,thresh=1e-6)\n",
    "\n",
    "\n",
    "text_ala_diss = Gen_diss_param_latex_table(filt_jumps,filt_symb_rates,filt_rates,'Alanine',ndecs=3)\n",
    "\n",
    "text_ala_basis = basis_to_latextable(Symb_basis,'Alanine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###saving in text files...\n",
    "with open(\"./data/jumps_ala.txt\", \"w\") as file:\n",
    "    file.write(text_ala_diss)\n",
    "\n",
    "with open(\"./data/basis_ala.txt\", \"w\") as file:\n",
    "    file.write(text_ala_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadMat = spio.loadmat('./data/NOESYdata_ALA_withGradients.mat',squeeze_me=True)\n",
    "\n",
    "t_grid1 = loadMat['p']['time_grid1'].item()\n",
    "t_grid2 = loadMat['p']['time_grid2'].item()\n",
    "\n",
    "R_ala = loadMat['p']['R'].item()\n",
    "H_ala = loadMat['p']['H'].item().toarray()\n",
    "\n",
    "rho0 = np.array(loadMat['p']['rho0'].item().toarray())\n",
    "\n",
    "coil = np.array(loadMat['p']['coil'].item())\n",
    "\n",
    "Lx = loadMat['p']['Lx'].item().toarray() \n",
    "Ly = loadMat['p']['Ly'].item().toarray() \n",
    "\n",
    "###Dynamical evolution for calculation of 2D spectra...\n",
    "Tpts1 = len(t_grid1)\n",
    "Tpts2 = len(t_grid2)\n",
    "\n",
    "#Parameters taken from Spinach script\n",
    "tmix = 1.0\n",
    "dt1 = 0.25e-3\n",
    "dt2 = 0.25e-3\n",
    "\n",
    "##Parameters for Fourier transform\n",
    "zerofill1 = 4096\n",
    "zerofill2 = 4096\n",
    "\n",
    "\n",
    "Spectrum, FID = GenNOESYSpectrum(H_ala,R_ala,Tpts1,Tpts2,rho0,coil,tmix,dt1,dt2,zerofill1,zerofill2,Lx,Ly)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save noesy data as a JSON file\n",
    "SaveNOESYData(tmix,t_grid1,t_grid2,rho0,H_ala,R_ala.toarray(),FID,'./data/ALA_NOESY_dat.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The TFG case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tfg = read_spinach_info(text4)\n",
    "\n",
    "basis_tfg = build_list_ISTs(data_tfg)\n",
    "prefacts,Symb_TFG_basis = build_symbolic_list_ISTs(data_tfg)\n",
    "\n",
    "#Normbasis = NormalizeBasis(basis,n_qubits=4,checkOrth=True) I have verified the orthonormalization of the basis\n",
    "Normbasis_tfg = NormalizeBasis(basis_tfg,n_qubits=4,checkOrth=False)\n",
    "Normbasis_tfg = np.array(Normbasis_tfg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "tc = 0.951e-9 # in seconds\n",
    "gammaF = 251814800\n",
    "B0 =9.3933\n",
    "\n",
    "w1 = -376415149.7831\n",
    "w2 = -376410090.0768\n",
    "w3 = -376411850.9715\n",
    "w4 = -376411792.9965\n",
    "\n",
    "\n",
    "freqs = np.array([w1,w2,w3,w4])\n",
    "\n",
    "coords = np.array([\n",
    "    [-0.0551 , -1.2087, -1.6523],\n",
    "    [-0.8604, -2.3200, -0.0624],\n",
    "    [-2.4464, -0.1125, -0.9776],\n",
    "    [-1.9914, -0.0836, 1.0743]\n",
    "])\n",
    "\n",
    "coords = coords*1e-10\n",
    "\n",
    "Nspins = 4\n",
    "\n",
    "#R_kiteALA_dev = Kite_relMat(2*np.pi*freqs,tc,coords,Nspins,gammaH,Sub_norm_basis)\n",
    "#R_kite_dev = KiteRelMatrixManyParallel(2*np.pi*freqs,tc,coords,Nspins,gammaH,Normbasis_ala,num_workers=20)\n",
    "\n",
    "#isotropic chemical shifts in ppm taken from the chemical shift tensors introduced in Spinach\n",
    "#-120.8361 -134.2763 -129.5988 -129.7528\n",
    "zeeman_scalar_1 = -120.8361\n",
    "zeeman_scalar_2 = -134.2763\n",
    "zeeman_scalar_3 = -129.5988\n",
    "zeeman_scalar_4 = -129.7528\n",
    "\n",
    "zeeman_scalars = [zeeman_scalar_1,zeeman_scalar_2,zeeman_scalar_3,zeeman_scalar_4]\n",
    "\n",
    "chem_shifts = get_chemical_shifts(gammaF,B0,zeeman_scalars)\n",
    "\n",
    "#list_jumps, list_damp_rates, list_dets=Get_Det_And_Rates(2*np.pi*freqs,tc,coords,Nspins,gammaF,chem_shifts)\n",
    "\n",
    "\n",
    "list_jumps, list_symb_rates, list_damp_rates, list_dets = Get_Det_And_Rates_latex(2*np.pi*freqs,tc,coords,Nspins,gammaF,chem_shifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt_jumps,filt_symb_rates,filt_rates = filter_jumps(list_jumps,list_symb_rates,list_damp_rates,list_dets,thresh=370)\n",
    "\n",
    "\n",
    "text_tfg_diss = Gen_diss_param_latex_table(filt_jumps,filt_symb_rates,filt_rates,'TFG',ndecs=3)\n",
    "\n",
    "text_tfg_basis = basis_to_latextable(Symb_basis,'TFG')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "###saving in text files...\n",
    "with open(\"./data/jumps_tfg.txt\", \"w\") as file:\n",
    "    file.write(text_tfg_diss)\n",
    "\n",
    "with open(\"./data/basis_tfg.txt\", \"w\") as file:\n",
    "    file.write(text_tfg_basis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "loadMat = spio.loadmat('./data/TFG_secular.mat',squeeze_me=True)\n",
    "\n",
    "t_grid1 = loadMat['p']['time_grid1'].item()\n",
    "t_grid2 = loadMat['p']['time_grid2'].item()\n",
    "R_tfg = loadMat['p']['R'].item()\n",
    "H_tfg = loadMat['p']['H'].item()\n",
    "\n",
    "rho0 = np.array(loadMat['p']['rho0'].item().toarray())\n",
    "\n",
    "coil = np.array(loadMat['p']['coil'].item())\n",
    "\n",
    "Lx = loadMat['p']['Lx'].item().toarray() \n",
    "Ly = loadMat['p']['Ly'].item().toarray() \n",
    "\n",
    "###Dynamical evolution for calculation of 2D spectra...\n",
    "Tpts1 = len(t_grid1)\n",
    "Tpts2 = len(t_grid2)\n",
    "\n",
    "\n",
    "\n",
    "#Parameters taken from Spinach script\n",
    "tmix = 0.5\n",
    "dt1 = 0.125e-3\n",
    "dt2 = 0.125e-3\n",
    "\n",
    "##Parameters for Fourier transform\n",
    "zerofill1 = 1024\n",
    "zerofill2 = 512\n",
    "\n",
    "\n",
    "#Calculation of the NOESY spectra using the reference relaxation matrix and the Kite and the jump-operator-truncated approach\n",
    "\n",
    "Ref_spec, FID = GenNOESYSpectrum(H_tfg,R_tfg,Tpts1,Tpts2,rho0,coil,tmix,dt1,dt2,zerofill1,zerofill2,Lx,Ly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "SaveNOESYData(tmix,t_grid1,t_grid2,rho0,H_tfg.toarray(),R_tfg,FID,'./data/TFG_NOESY_dat.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BQSKit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
